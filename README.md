# wb_test_task

1. Как запустить ETL вручную

	Перейдите во вкладку Actions для даного репозитория
	Выберите workflow "Run ETL Pipeline"
	Откройте выпадающий список "Run workflow" в правом верхнем углу
	Нажмите	зеленую кнопку "Run workflow"

2. Как работает CI/CD

	Проект использует GitHub Actions для CI/CD. Процесс выглядит так:
	При каждом push'е или ручном запуске:
	GitHub запускает pipeline из .github/workflows/etl_pipeline.yml

	Шаги включают:
	Установка Python
	Установка зависимостей
	Запуск скрипта etl.py
	Сохранение выходных файлов (Excel, БД, графики) как GitHub Artifacts

	Таким образом:
	CI (непрерывная интеграция): проверка кода и воспроизводимость
	CD (непрерывная доставка): автоматическое формирование результатов (можно расширить до выгрузки в S3 и т.п.)

3. Как бы вы адаптировали решение под 100+ млн строк?
	
	Для обработки такого объема нужны распределённые и масштабируемые решения.

	3.1. Какие технологии замените/добавите?
	Назначение				Сейчас				При масштабировании
	Источник данных			CSV-файл			Amazon S3, GCS
	Обработка данных		Pandas (в памяти)	Apache Spark или Dask
	Оркестрация				GitHub Actions		Apache Airflow, Prefect, Dagster
	Хранилище				SQLite				PostgreSQL, Redshift, BigQuery
	Отчёты / графики		Excel / Matplotlib	PowerBI, Tableau, Looker
	
	3.2. Какую архитектуру ETL предложите?
	
	           +-------------+
               |  CSV / S3   |
               +------+------+
                      |
                      ▼
                +-----+------+
                | Spark/Dask |
                +-----+------+
                      |
         +------------+-------------+
         |                          |
   +-----+-----+            +-------+--------+
   | Delta Lake|            | Analytical DB  |
   +-----------+            +----------------+
                      ▼
           +----------+-----------+
           |  BI-инструменты      |
           +----------------------+
	
	3.3. Какие метрики мониторинга ETL вы бы внедрили?
	Метрика								Назначение
	Кол-во строк на входе/выходе		Отслеживание потерь
	Процент пустых значений и N/A		Контроль качества данных
	Время обработки каждого этапа		Оптимизация производительности
	Задержка по времени события			Актуальность данных
	Количество ошибок и сбоев			Триггер алертов и повторных запусков
	
	Инструменты:
	Airflow logs + Grafana
	Встроенные метрики Prefect/Dagster
	
	3.4. Где будут храниться входные и выходные данные?
	CSV и SQLite не подходят для масштабов в сотни миллионов строк.
	
	Входные данные:
	Хранятся в Amazon S3 или Google Cloud Storage
	Организованы по датам: s3://bucket/trades/year=2025/month=07/day=20/

	Выходные данные:
	Хранятся в:
	Parquet или Delta Lake — эффективно и дешево
	Redshift, Snowflake, BigQuery — аналитические БД
	PostgreSQL — для средних объемов