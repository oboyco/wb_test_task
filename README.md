NOTE: LOGS and TESTS were missed by design as there were no time and no requirements. We can talk about it during the meeting

## 1. Как запустить ETL вручную

1. Перейдите во вкладку **Actions** данного репозитория
2. Выберите workflow **Run ETL Pipeline**
3. Откройте выпадающий список **Run workflow** в правом верхнем углу
4. Нажмите зелёную кнопку **Run workflow**
5. Проверьте результаты в папке `output`

---

## 2. Как работает CI/CD

Проект использует **GitHub Actions** для CI/CD. 

### Автоматический запуск:

- При каждом `push` или ручном запуске:
  - GitHub запускает pipeline: `.github/workflows/etl_pipeline.yml`

### Шаги пайплайна:

- Установка Python
- Установка зависимостей
- Запуск скрипта `etl.py`
- Сохранение выходных файлов (Excel, БД, графики) как GitHub Artifacts

---

## 3. Масштабирование для 100+ млн строк

Обработка больших данных требует масштабируемых решений.

### 3.1. Какие технологии заменить/добавить?

| Назначение         | Сейчас              | При масштабировании                |
|--------------------|---------------------|------------------------------------|
| Источник данных    | CSV-файл            | Amazon S3 / Google Cloud Storage   |
| Обработка данных   | Pandas              | Apache Spark / Dask                |
| Оркестрация        | GitHub Actions      | Apache Airflow / Prefect / Dagster|
| Хранилище          | SQLite              | PostgreSQL / Redshift / BigQuery   |
| Визуализация       | Excel / Matplotlib  | Power BI / Tableau / Looker        |
	
### 3.2. Какую архитектуру ETL предложите?
	
```
       +-------------+
       |  CSV / S3   |
       +------+------+
              |
              ▼
        +-----+------+
        | Spark/Dask |
        +-----+------+
              |
   +----------+-----------+
   |                      |
+--+-----------+    +-----+------------+
| Delta Lake   |    | Analytical DB   |
+--------------+    +-----------------+
              ▼
   +----------+-----------+
   |  BI-инструменты      |
   +----------------------+
```

### 3.3. Какие метрики мониторинга ETL вы бы внедрили?

| Метрика                           | Назначение                         |
|----------------------------------|------------------------------------|
| Кол-во строк на входе/выходе     | Отслеживание потерь                |
| Процент пустых значений и N/A    | Контроль качества данных           |
| Время обработки каждого этапа    | Оптимизация производительности     |
| Задержка по времени события      | Актуальность данных                |
| Количество ошибок и сбоев        | Триггер алертов и повторных запусков |

#### Инструменты:
- **Airflow logs** + **Grafana**
- Встроенные метрики **Prefect** / **Dagster**

### 3.4. Где будут храниться входные и выходные данные?

CSV и SQLite не подходят для масштабов в сотни миллионов строк.

#### Входные данные:
- Хранятся в: **Amazon S3** или **Google Cloud Storage**
- Организованы по датам:  
  `s3://bucket/trades/year=2025/month=07/day=20/`

#### Выходные данные:
- Хранятся в:
  - **Parquet** или **Delta Lake** — эффективно и дешево
  - **Redshift**, **Snowflake**, **BigQuery** — аналитические БД
  - **PostgreSQL** — для средних объёмов
